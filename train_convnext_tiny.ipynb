{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mid_Test MLOps (Train w Convnext tiny, stream with streamlit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2a3ea9",
      "metadata": {},
      "source": [
        "Connect to mlflow (later streamlit with docker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ba175afe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting MLFlow server...\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def start_mlflow_server():\n",
        "    process = subprocess.Popen([\n",
        "        sys.executable, \"-m\", \"mlflow\", \"server\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--port\", \"5600\",\n",
        "        \"--backend-store-uri\", \"sqlite:///mlflow.db\",\n",
        "        \"--default-artifact-root\", \"./mlruns\"\n",
        "    ])\n",
        "    return process\n",
        "\n",
        "print(\"Starting MLFlow server...\")\n",
        "mlflow_process = start_mlflow_server()\n",
        "mlflow.set_tracking_uri(\"http://127.0.0.1:5600\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6e8f9f",
      "metadata": {},
      "source": [
        "Classic Machine Learning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c75a76b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\owen\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#Import library\n",
        "import os, math\n",
        "import torch\n",
        "from torch import nn, amp\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import RandAugment\n",
        "from pathlib import Path\n",
        "import copy, optuna\n",
        "from optuna.pruners import MedianPruner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Path\n",
        "Base_Path   = r\"C:\\Users\\owen\\MLOPSMid\"\n",
        "Train_Path  = os.path.join(Base_Path, \"train\", \"train\")\n",
        "Test_Path    = os.path.join(Base_Path, \"test\", \"test\")\n",
        "\n",
        "OUT_DIR       = \"./weights\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ca967975",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "#Variable     -- will not be tuned\n",
        "BATCH_SIZE    = 32\n",
        "NUM_WORKERS   = 4\n",
        "VAL_SIZE      = 0.2\n",
        "SEED          = 42\n",
        "IMAGE_SIZE    = 224\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "EPOCHS        = 20\n",
        "WARMUP_EPOCHS = 4\n",
        "Device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype         = torch.bfloat16 if Device.type == \"cuda\" else torch.float32 #Efisiensi GPU :v\n",
        "\n",
        "print(Device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#preprocessing function\n",
        "def build_transforms(image_size: int):\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.7, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        RandAugment(num_ops=2, magnitude=9),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "        transforms.RandomErasing(p=0.25, scale=(0.02, 0.15), value=\"random\")\n",
        "    ])\n",
        "    val_tf = transforms.Compose([\n",
        "        transforms.Resize(int(image_size * 1.15)),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "#spliting function --> return idx\n",
        "def index_split(dataset: datasets.ImageFolder, val_size: float):\n",
        "    y = [dataset.samples[i][1] for i in range(len(dataset.samples))]\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=SEED)\n",
        "    train_idx, val_idx = next(splitter.split(range(len(y)), y))\n",
        "    return train_idx, val_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jumlah kelas: 5, beranggotakan: ['Bacterial Pneumonia', 'Corona Virus Disease', 'Normal', 'Tuberculosis', 'Viral Pneumonia']\n"
          ]
        }
      ],
      "source": [
        "# Load dataset train\n",
        "train = Path(Train_Path)\n",
        "\n",
        "temp = datasets.ImageFolder(str(train), transform=transforms.ToTensor())\n",
        "class_names = temp.classes\n",
        "num_classes = len(class_names)\n",
        "print(f\"Jumlah kelas: {num_classes}, beranggotakan: {class_names}\")\n",
        "\n",
        "train_tf, val_tf = build_transforms(IMAGE_SIZE)\n",
        "full_train_ds = datasets.ImageFolder(str(train), transform=train_tf)\n",
        "full_val_ds   = datasets.ImageFolder(str(train), transform=val_tf)\n",
        "\n",
        "# Splitting according to index returned by the function, so far full_train_ds = full_val_ds\n",
        "train_idx, val_idx = index_split(full_train_ds, VAL_SIZE)\n",
        "train_ds = Subset(full_train_ds, train_idx) #Taking only the train slices according to train_idx\n",
        "val_ds   = Subset(full_val_ds,   val_idx) #Taking only the validation slices according to val_idx\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=(NUM_WORKERS > 0)\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=(NUM_WORKERS > 0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3c281348",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset test\n",
        "test = Path(Test_Path)\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.Resize(int(IMAGE_SIZE * 1.15)),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "test_ds = datasets.ImageFolder(str(test), transform=test_tf)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        "    persistent_workers=(NUM_WORKERS > 0)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ConvNeXt(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2dNormActivation(\n",
            "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
            "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
            "      )\n",
            "      (1): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
            "      )\n",
            "      (2): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
            "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
            "      )\n",
            "      (1): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
            "      )\n",
            "      (2): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
            "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
            "      )\n",
            "      (1): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
            "      )\n",
            "      (2): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
            "      )\n",
            "      (3): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
            "      )\n",
            "      (4): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
            "      )\n",
            "      (5): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
            "      )\n",
            "      (6): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
            "      )\n",
            "      (7): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
            "      )\n",
            "      (8): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
            "      )\n",
            "      (1): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
            "      )\n",
            "      (2): CNBlock(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
            "          (1): Permute()\n",
            "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (4): GELU(approximate='none')\n",
            "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (6): Permute()\n",
            "        )\n",
            "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (classifier): Sequential(\n",
            "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=768, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Model defining\n",
        "weights = ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
        "model = convnext_tiny(weights=weights)\n",
        "in_features = model.classifier[2].in_features\n",
        "model.classifier[2] = nn.Linear(in_features, num_classes)\n",
        "model.to(Device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e5fafc8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "class WarmupCosineLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"Linear warmup for WARMUP_EPOCHS, then cosine decay to 0 until EPOCHS.\"\"\"\n",
        "    def __init__(self, optimizer, warmup_epochs, max_epochs, last_epoch=-1):\n",
        "        self.warmup_epochs = max(0, int(warmup_epochs))\n",
        "        self.max_epochs = max_epochs\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "    def get_lr(self):\n",
        "        e = self.last_epoch\n",
        "        if e < self.warmup_epochs:\n",
        "            warm = (e + 1) / max(1, self.warmup_epochs)\n",
        "            return [base_lr * warm for base_lr in self.base_lrs]\n",
        "        t = (e - self.warmup_epochs) / max(1, self.max_epochs - self.warmup_epochs)\n",
        "        cosine = 0.5 * (1 + math.cos(math.pi * t))\n",
        "        return [base_lr * cosine for base_lr in self.base_lrs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy function\n",
        "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == targets).float().mean().item()\n",
        "\n",
        "# With this function, we could control what each epochs do\n",
        "def train_one_epoch(model, loader, criterion, optimizer, scaler, device):\n",
        "    model.train()                                      #Normal train\n",
        "    loss_sum, acc_sum, n = 0.0, 0.0, 0\n",
        "    pbar = tqdm(loader, leave=False, desc=\"Train\")\n",
        "    for step, (x, y) in enumerate(pbar, 1):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "        if y.dtype != torch.long:\n",
        "            y = y.long()\n",
        "        optimizer.zero_grad(set_to_none=True)          #reset gradient sebelum epoch berikutnya\n",
        "        with amp.autocast(device_type=device.type, dtype=dtype):           #Penggunaan bf16\n",
        "            logits = model(x)\n",
        "            C = logits.size(1)\n",
        "            loss = criterion(logits, y)                #Loss\n",
        "        scaler.scale(loss).backward()                  #backward mendapatkan ulang gradient\n",
        "        scaler.step(optimizer)                         #perubahan pada weight model\n",
        "        scaler.update()\n",
        "        acc = accuracy_from_logits(logits, y)          #Accuracy\n",
        "        loss_sum += loss.item(); acc_sum += acc; n += 1\n",
        "        pbar.set_postfix(loss=f\"{loss_sum/n:.4f}\", acc=f\"{acc_sum/n:.4f}\") #live value progress update\n",
        "    return loss_sum / max(1, n), acc_sum / max(1, n)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()                                       #Normal evaluate\n",
        "    loss_sum, acc_sum, n = 0.0, 0.0, 0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "        if y.dtype != torch.long:\n",
        "            y = y.long()\n",
        "        with amp.autocast(device_type=device.type, dtype=dtype):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)                #Loss\n",
        "        acc = accuracy_from_logits(logits, y)          #Accuracy\n",
        "        loss_sum += loss.item(); acc_sum += acc; n += 1\n",
        "    return loss_sum / max(1, n), acc_sum / max(1, n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "18c55024",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-14 10:49:03,448] A new study created in memory with name: convnext_opt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 0] lr=2.23e-05 wd=8.8e-06 ls=0.16 warmup=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 01/20 | lr 2.23e-05 | train 1.0114/0.7381 | val 0.8150/0.8508 | 205.9s\n",
            "[INFO][T0] Saved best to ./weights\\trial0_ConvNext_Tiny.pt (val_acc=0.8508)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 02/20 | lr 2.21e-05 | train 0.8119/0.8606 | val 0.7977/0.8556 | 164.9s\n",
            "[INFO][T0] Saved best to ./weights\\trial0_ConvNext_Tiny.pt (val_acc=0.8556)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 03/20 | lr 2.17e-05 | train 0.7735/0.8804 | val 0.7539/0.8903 | 164.3s\n",
            "[INFO][T0] Saved best to ./weights\\trial0_ConvNext_Tiny.pt (val_acc=0.8903)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 04/20 | lr 2.09e-05 | train 0.7563/0.8905 | val 0.7561/0.8844 | 165.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 05/20 | lr 1.99e-05 | train 0.7325/0.9046 | val 0.7679/0.8803 | 165.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 06/20 | lr 1.87e-05 | train 0.7242/0.9089 | val 0.7295/0.8984 | 165.1s\n",
            "[INFO][T0] Saved best to ./weights\\trial0_ConvNext_Tiny.pt (val_acc=0.8984)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 07/20 | lr 1.72e-05 | train 0.7121/0.9143 | val 0.7283/0.9033 | 165.4s\n",
            "[INFO][T0] Saved best to ./weights\\trial0_ConvNext_Tiny.pt (val_acc=0.9033)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 08/20 | lr 1.56e-05 | train 0.6974/0.9270 | val 0.7490/0.8936 | 756.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 09/20 | lr 1.39e-05 | train 0.6936/0.9277 | val 0.7330/0.8994 | 37.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 10/20 | lr 1.21e-05 | train 0.6827/0.9363 | val 0.7522/0.8854 | 37.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 11/20 | lr 1.02e-05 | train 0.6803/0.9361 | val 0.7208/0.9068 | 35.4s\n",
            "[INFO][T0] Saved best to ./weights\\trial0_ConvNext_Tiny.pt (val_acc=0.9068)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 12/20 | lr 8.40e-06 | train 0.6713/0.9423 | val 0.7254/0.9058 | 35.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 13/20 | lr 6.66e-06 | train 0.6658/0.9486 | val 0.7217/0.9068 | 35.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 14/20 | lr 5.05e-06 | train 0.6668/0.9460 | val 0.7101/0.9166 | 34.8s\n",
            "[INFO][T0] Saved best to ./weights\\trial0_ConvNext_Tiny.pt (val_acc=0.9166)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 15/20 | lr 3.59e-06 | train 0.6598/0.9511 | val 0.7175/0.9107 | 36.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 16/20 | lr 2.35e-06 | train 0.6532/0.9556 | val 0.7152/0.9125 | 36.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 17/20 | lr 1.34e-06 | train 0.6519/0.9564 | val 0.7117/0.9133 | 34.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 18/20 | lr 6.04e-07 | train 0.6529/0.9546 | val 0.7137/0.9117 | 34.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 19/20 | lr 1.52e-07 | train 0.6509/0.9564 | val 0.7151/0.9117 | 34.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-14 11:28:48,261] Trial 0 finished with value: 0.9166362081703386 and parameters: {'lr': 2.227745394015427e-05, 'weight_decay': 8.789300574819898e-06, 'label_smoothing': 0.16279819806760748, 'warmup_epochs': 1}. Best is trial 0 with value: 0.9166362081703386.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T0] Epoch 20/20 | lr 0.00e+00 | train 0.6479/0.9597 | val 0.7137/0.9125 | 34.5s\n",
            "[TRIAL 0] Best val acc 0.9166 at epoch 14.\n",
            "[TRIAL 1] lr=2.30e-04 wd=1.3e-06 ls=0.10 warmup=4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 01/20 | lr 1.15e-04 | train 0.8121/0.7801 | val 0.6463/0.8656 | 34.3s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.8656)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 02/20 | lr 1.73e-04 | train 0.6616/0.8600 | val 0.6490/0.8525 | 35.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 03/20 | lr 2.30e-04 | train 0.6480/0.8662 | val 0.6092/0.8821 | 38.8s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.8821)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 04/20 | lr 2.30e-04 | train 0.6471/0.8680 | val 0.6048/0.8985 | 35.5s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.8985)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 05/20 | lr 2.28e-04 | train 0.6055/0.8954 | val 0.6300/0.8671 | 35.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 06/20 | lr 2.21e-04 | train 0.5921/0.8966 | val 0.6115/0.8953 | 34.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 07/20 | lr 2.11e-04 | train 0.5674/0.9096 | val 0.6238/0.8918 | 35.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 08/20 | lr 1.96e-04 | train 0.5484/0.9246 | val 0.6272/0.8780 | 34.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 09/20 | lr 1.79e-04 | train 0.5273/0.9351 | val 0.5814/0.9074 | 34.7s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.9074)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 10/20 | lr 1.59e-04 | train 0.5105/0.9451 | val 0.5821/0.8994 | 34.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 11/20 | lr 1.37e-04 | train 0.4999/0.9492 | val 0.5800/0.9086 | 34.7s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.9086)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 12/20 | lr 1.15e-04 | train 0.4835/0.9589 | val 0.5822/0.9135 | 34.1s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.9135)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 13/20 | lr 9.26e-05 | train 0.4810/0.9613 | val 0.5972/0.9035 | 37.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 14/20 | lr 7.10e-05 | train 0.4603/0.9716 | val 0.6052/0.8967 | 37.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 15/20 | lr 5.11e-05 | train 0.4420/0.9815 | val 0.5765/0.9168 | 38.6s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.9168)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 16/20 | lr 3.37e-05 | train 0.4370/0.9833 | val 0.5701/0.9225 | 34.9s\n",
            "[INFO][T1] Saved best to ./weights\\trial1_ConvNext_Tiny.pt (val_acc=0.9225)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 17/20 | lr 1.94e-05 | train 0.4305/0.9891 | val 0.5790/0.9225 | 35.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 18/20 | lr 8.75e-06 | train 0.4253/0.9893 | val 0.5759/0.9193 | 34.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 19/20 | lr 2.21e-06 | train 0.4218/0.9918 | val 0.5767/0.9201 | 35.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-14 11:40:46,559] Trial 1 finished with value: 0.9225450785536515 and parameters: {'lr': 0.00023000700364804216, 'weight_decay': 1.2854956073548836e-06, 'label_smoothing': 0.10384556115948224, 'warmup_epochs': 4}. Best is trial 1 with value: 0.9225450785536515.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T1] Epoch 20/20 | lr 0.00e+00 | train 0.4194/0.9932 | val 0.5724/0.9201 | 34.6s\n",
            "[TRIAL 1] Best val acc 0.9225 at epoch 16.\n",
            "[TRIAL 2] lr=6.04e-05 wd=3.4e-04 ls=0.16 warmup=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 01/20 | lr 6.04e-05 | train 0.9625/0.7626 | val 0.7926/0.8640 | 34.6s\n",
            "[INFO][T2] Saved best to ./weights\\trial2_ConvNext_Tiny.pt (val_acc=0.8640)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 02/20 | lr 6.04e-05 | train 0.7930/0.8569 | val 0.8623/0.8249 | 34.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 03/20 | lr 5.99e-05 | train 0.7453/0.8849 | val 0.7330/0.8879 | 34.2s\n",
            "[INFO][T2] Saved best to ./weights\\trial2_ConvNext_Tiny.pt (val_acc=0.8879)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 04/20 | lr 5.86e-05 | train 0.7221/0.8978 | val 0.7288/0.8847 | 35.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 05/20 | lr 5.64e-05 | train 0.6984/0.9147 | val 0.7173/0.9059 | 34.2s\n",
            "[INFO][T2] Saved best to ./weights\\trial2_ConvNext_Tiny.pt (val_acc=0.9059)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 06/20 | lr 5.33e-05 | train 0.6894/0.9199 | val 0.7176/0.8969 | 34.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 07/20 | lr 4.96e-05 | train 0.6722/0.9289 | val 0.7596/0.8828 | 34.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 08/20 | lr 4.53e-05 | train 0.6672/0.9367 | val 0.7144/0.9002 | 34.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 09/20 | lr 4.05e-05 | train 0.6491/0.9461 | val 0.6942/0.9158 | 35.0s\n",
            "[INFO][T2] Saved best to ./weights\\trial2_ConvNext_Tiny.pt (val_acc=0.9158)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 10/20 | lr 3.54e-05 | train 0.6390/0.9502 | val 0.7047/0.9109 | 34.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 11/20 | lr 3.02e-05 | train 0.6248/0.9638 | val 0.7207/0.9010 | 34.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 12/20 | lr 2.50e-05 | train 0.6197/0.9638 | val 0.7369/0.9033 | 34.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 13/20 | lr 1.99e-05 | train 0.6087/0.9727 | val 0.6993/0.9183 | 34.4s\n",
            "[INFO][T2] Saved best to ./weights\\trial2_ConvNext_Tiny.pt (val_acc=0.9183)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 14/20 | lr 1.51e-05 | train 0.6055/0.9747 | val 0.6958/0.9224 | 35.4s\n",
            "[INFO][T2] Saved best to ./weights\\trial2_ConvNext_Tiny.pt (val_acc=0.9224)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 15/20 | lr 1.08e-05 | train 0.5959/0.9805 | val 0.7029/0.9191 | 34.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 16/20 | lr 7.07e-06 | train 0.5938/0.9825 | val 0.7174/0.9125 | 34.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 17/20 | lr 4.05e-06 | train 0.5929/0.9813 | val 0.7042/0.9184 | 34.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 18/20 | lr 1.82e-06 | train 0.5887/0.9848 | val 0.7077/0.9184 | 34.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 19/20 | lr 4.59e-07 | train 0.5877/0.9856 | val 0.7029/0.9193 | 34.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-14 11:52:22,036] Trial 2 finished with value: 0.9223927871177071 and parameters: {'lr': 6.03974035063107e-05, 'weight_decay': 0.0003361444320401668, 'label_smoothing': 0.15730112957081632, 'warmup_epochs': 2}. Best is trial 1 with value: 0.9225450785536515.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[T2] Epoch 20/20 | lr 0.00e+00 | train 0.5835/0.9866 | val 0.7046/0.9201 | 34.4s\n",
            "[TRIAL 2] Best val acc 0.9224 at epoch 14.\n",
            "=== Best Trial ===\n",
            "val_acc: 0.9225450785536515\n",
            "params: {'lr': 0.00023000700364804216, 'weight_decay': 1.2854956073548836e-06, 'label_smoothing': 0.10384556115948224, 'warmup_epochs': 4}\n",
            "[INFO] Copied best checkpoint from trial 1 to ./weights\\ConvNext_Tiny.pt\n"
          ]
        }
      ],
      "source": [
        "base_model_state = copy.deepcopy(model.state_dict())\n",
        "best_val_acc, best_epoch = 0.0, -1\n",
        "ckpt_path = os.path.join(OUT_DIR, \"ConvNext_Tiny.pt\")\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "    #Hyperparameter tuning\n",
        "    lr_suggest  = trial.suggest_float(\"lr\", 1e-5, 8e-4, log=True)\n",
        "    wd_suggest  = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "    ls_suggest  = trial.suggest_float(\"label_smoothing\", 0.0, 0.2)\n",
        "    warmup_sugg = trial.suggest_int(\"warmup_epochs\", 1, WARMUP_EPOCHS)\n",
        "\n",
        "    # Model load\n",
        "    model.load_state_dict(base_model_state)\n",
        "    model.to(Device)\n",
        "\n",
        "    local_criterion = torch.nn.CrossEntropyLoss(label_smoothing=ls_suggest)\n",
        "    local_optimizer = torch.optim.AdamW(model.parameters(), lr=lr_suggest, weight_decay=wd_suggest)\n",
        "    local_scheduler = WarmupCosineLR(local_optimizer, warmup_epochs=warmup_sugg, max_epochs=EPOCHS)\n",
        "    local_scaler = amp.GradScaler(\"cuda\" if Device.type == \"cuda\" else \"cpu\")\n",
        "\n",
        "    best_val_acc, best_epoch = 0.0, -1\n",
        "    ckpt_path_trial = os.path.join(OUT_DIR, f\"trial{trial.number}_ConvNext_Tiny.pt\")\n",
        "\n",
        "    print(f\"[TRIAL {trial.number}] lr={lr_suggest:.2e} wd={wd_suggest:.1e} ls={ls_suggest:.2f} warmup={warmup_sugg}\")\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "        # Normal train\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, local_criterion, local_optimizer, local_scaler, Device)\n",
        "        # Normal evaluate\n",
        "        va_loss, va_acc = validate(model, val_loader, local_criterion, Device)\n",
        "        # Refresh scheduler\n",
        "        local_scheduler.step()\n",
        "        # Used learning rate\n",
        "        lr_now = local_optimizer.param_groups[0][\"lr\"]\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        print(f\"[T{trial.number}] Epoch {epoch:02d}/{EPOCHS} | lr {lr_now:.2e} | \"\n",
        "              f\"train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f} | {dt:.1f}s\")\n",
        "\n",
        "        trial.report(float(va_acc), step=epoch)\n",
        "        if trial.should_prune():\n",
        "            print(f\"[TRIAL {trial.number}] pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # Save best checkpoint untuk trial ini\n",
        "        if va_acc > best_val_acc:\n",
        "            best_val_acc, best_epoch = va_acc, epoch\n",
        "            torch.save({\n",
        "                \"epoch\": best_epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": local_optimizer.state_dict(),\n",
        "                \"val_acc\": best_val_acc,\n",
        "                \"class_names\": class_names,\n",
        "                \"image_size\": IMAGE_SIZE,\n",
        "                \"config\": {\n",
        "                    \"LR\": lr_suggest, \"WEIGHT_DECAY\": wd_suggest,\n",
        "                    \"EPOCHS\": EPOCHS, \"WARMUP_EPOCHS\": warmup_sugg,\n",
        "                    \"LABEL_SMOOTH\": ls_suggest\n",
        "                }\n",
        "            }, ckpt_path_trial)\n",
        "            print(f\"[INFO][T{trial.number}] Saved best to {ckpt_path_trial} (val_acc={best_val_acc:.4f})\")\n",
        "\n",
        "    print(f\"[TRIAL {trial.number}] Best val acc {best_val_acc:.4f} at epoch {best_epoch}.\")\n",
        "    return float(best_val_acc)\n",
        "\n",
        "# bikin study dan jalanin trial\n",
        "study = optuna.create_study(\n",
        "    study_name=\"convnext_opt\",\n",
        "    direction=\"maximize\",\n",
        "    pruner=MedianPruner(n_startup_trials=4, n_warmup_steps=3)\n",
        ")\n",
        "study.optimize(objective, n_trials=3, gc_after_trial=True)\n",
        "\n",
        "print(\"=== Best Trial ===\")\n",
        "print(\"val_acc:\", study.best_value)\n",
        "print(\"params:\", study.best_params)\n",
        "\n",
        "# copy checkpoint terbaik ke nama default kamu (biar kompatibel dengan pipeline berikutnya)\n",
        "best_trial_num = study.best_trial.number\n",
        "best_trial_ckpt = os.path.join(OUT_DIR, f\"trial{best_trial_num}_ConvNext_Tiny.pt\")\n",
        "if os.path.isfile(best_trial_ckpt):\n",
        "    import shutil\n",
        "    shutil.copyfile(best_trial_ckpt, ckpt_path)\n",
        "    print(f\"[INFO] Copied best checkpoint from trial {best_trial_num} to {ckpt_path}\")\n",
        "else:\n",
        "    print(\"[WARN] Best trial checkpoint not found; leaving original path untouched.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8e6371",
      "metadata": {},
      "source": [
        "Store model on Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\owen\\AppData\\Local\\Temp\\ipykernel_46828\\2213047478.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(r\"C:\\Users\\owen\\MLOPSMid\\weights\\ConvNext_Tiny.pt\", map_location=Device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test accuracy: 91.72%\n",
            "\n",
            "Classification report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            " Bacterial Pneumonia       0.86      0.78      0.82       401\n",
            "Corona Virus Disease       0.99      1.00      0.99       406\n",
            "              Normal       0.97      0.96      0.96       402\n",
            "        Tuberculosis       1.00      1.00      1.00       406\n",
            "     Viral Pneumonia       0.77      0.85      0.81       401\n",
            "\n",
            "            accuracy                           0.92      2016\n",
            "           macro avg       0.92      0.92      0.92      2016\n",
            "        weighted avg       0.92      0.92      0.92      2016\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "ckpt = torch.load(r\"C:\\Users\\owen\\MLOPSMid\\weights\\ConvNext_Tiny.pt\", map_location=Device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_on_test(model, loader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    use_amp = hasattr(torch, \"amp\") and device.type == \"cuda\"\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        if use_amp:\n",
        "            with amp.autocast(device_type=device.type, dtype=dtype):\n",
        "                logits = model(xb)\n",
        "        else:\n",
        "            logits = model(xb)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(yb.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f\"\\nTest accuracy: {acc * 100:.2f}%\\n\")\n",
        "    print(\"Classification report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    return cm, all_labels, all_preds\n",
        "\n",
        "cm, y_true, y_pred = evaluate_on_test(model, test_loader, Device, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca5e121",
      "metadata": {},
      "source": [
        "Result: Accuracy is 0.9172 which rounded to 0.92 from best model with:\n",
        "\n",
        "params: {'lr': 0.00023000700364804216,\n",
        "\n",
        "'weight_decay': 1.2854956073548836e-06,\n",
        "\n",
        "'label_smoothing': 0.10384556115948224,\n",
        "\n",
        "'warmup_epochs': 4}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7696cc8",
      "metadata": {},
      "source": [
        "# Store to mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d681bb72",
      "metadata": {},
      "outputs": [],
      "source": [
        "preproc_dir = Path(\"preprocessing\")\n",
        "preproc_dir.mkdir(exist_ok=True)\n",
        "\n",
        "train_tf_path = preproc_dir / \"train_tf.txt\"\n",
        "with open(train_tf_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(str(train_tf))\n",
        "\n",
        "mlflow.log_artifact(str(train_tf_path), artifact_path=\"preprocessing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "698d85a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\owen\\AppData\\Local\\Temp\\ipykernel_13220\\1892898339.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(r\"C:\\Users\\owen\\MLOPSMid\\weights\\trial1_ConvNext_Tiny.pt\", map_location=Device)\n",
            "2025/11/14 18:49:10 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/11/14 18:49:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<mlflow.models.model.ModelInfo at 0x2666e522170>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import mlflow.pytorch\n",
        "\n",
        "ckpt = torch.load(r\"C:\\Users\\owen\\MLOPSMid\\weights\\trial1_ConvNext_Tiny.pt\", map_location=Device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.to(Device)\n",
        "model.eval()\n",
        "\n",
        "mlflow.pytorch.log_model(\n",
        "    model,\n",
        "    artifact_path=\"pytorch_model\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc28e271",
      "metadata": {},
      "source": [
        "# Stream in Streamlit & Containerization with Docker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a1fa11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting predict.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.cuda import amp\n",
        "from torchvision.models import convnext_tiny\n",
        "\n",
        "Device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.bfloat16 if Device.type == \"cuda\" else torch.float32\n",
        "\n",
        "\n",
        "CKPT_PATH = r\"C:\\Users\\owen\\MLOPSMid\\weights\\ConvNext_Tiny.pt\"\n",
        "IMAGE_SIZE = 224\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "class_names = [\n",
        "    \"Bacterial Pneumonia\",\n",
        "    \"Corona Virus Disease\",\n",
        "    \"Normal\",\n",
        "    \"Tuberculosis\",\n",
        "    \"Viral Pneumonia\"\n",
        "]\n",
        "num_classes = len(class_names)\n",
        "\n",
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize(int(IMAGE_SIZE * 1.15)),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "model = convnext_tiny(weights=None)\n",
        "model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=Device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.to(Device)\n",
        "model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one(img_pil: Image.Image, model = model):\n",
        "    img = eval_tf(img_pil).unsqueeze(0).to(Device)\n",
        "\n",
        "    logits = model(img)\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    pred_idx = int(probs.argmax())\n",
        "    pred_class = class_names[pred_idx]\n",
        "    confidence = float(probs[pred_idx])\n",
        "\n",
        "    return pred_class, confidence, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6328e63b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from predict import predict_one, class_names\n",
        "\n",
        "st.title(\"Mid-Test MLOps X-ray image dissease classification\")\n",
        "st.write(f\"Upload an image, Ill classify it as either {class_names}\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload here (one image only)\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    img = Image.open(uploaded_file).convert(\"RGB\")\n",
        "    st.image(img, caption=\"Uploaded!\", use_column_width=True)\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        with st.spinner(\"Loading, sabar ya\"):\n",
        "            pred_class, confidence, probs = predict_one(img)\n",
        "\n",
        "        st.subheader(\"Prediction result\")\n",
        "        st.write(f\"Predicted class: {pred_class}\")\n",
        "        st.write(f\"Confidence: {confidence * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "746044c7",
      "metadata": {},
      "source": [
        "better run in terminal, no need to rerun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325ed565",
      "metadata": {},
      "outputs": [],
      "source": [
        "# docker pull python:3.10-slim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6da4de88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# docker build -t mlops-mid-app ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf642deb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# docker run --rm -p 8501:8501 mlops-mid-app"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a46eb3e8",
      "metadata": {},
      "source": [
        "run this to deploy my program, first u need to login first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3545b19",
      "metadata": {},
      "outputs": [],
      "source": [
        "#docker login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fb1d49",
      "metadata": {},
      "outputs": [],
      "source": [
        "# docker pull owenputra/mlops-mid-app:latest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3b112b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# docker run -p 8501:8501 owenputra/mlops-mid-app:latest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3077bee2",
      "metadata": {},
      "source": [
        "please open this once u run:http://localhost:8501"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
